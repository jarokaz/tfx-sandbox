{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YSXq_Y36c_g"
   },
   "source": [
    "# Training and Serving a TensorFlow Model to AI Platform Prediction\n",
    "\n",
    "This notebooks trains a TensorFlow classification model locally, using the Keras API, and deploys the model to AI Platform for online prediction. The AI Platform model service is also configered to enable request-response logging to BigQuery.\n",
    "\n",
    "Note that the aim is to build and deploy a **minimal model** to showcase the AI Platform **logging capabilities**, which enable **skew detection** on the serving data and the produced predictions.\n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "1. Prepare the data and generate metadata \n",
    "2. Train and evaluate, a TensorFlow classification model using Keras API\n",
    "3. Export the trained model as a SavedModel for serving\n",
    "4. Deploy the trained model to AI Platform Prediction \n",
    "5. Enabled request-response logging to BigQuery\n",
    "6. Parse and query logs from BigQuery\n",
    "\n",
    "\n",
    "This example uses **TensorFlow 2.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SthW6Y1F85SH"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4pR1SEo6Tra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TF version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCHY6HdE-wmw"
   },
   "source": [
    "**Set the following variables to match your GCP environment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVhekpMo9WqV"
   },
   "outputs": [],
   "source": [
    "PROJECT = 'mlops-dev-env'\n",
    "BUCKET =  'mlops-dev-workspace'\n",
    "GCS_DATA_LOCATION = 'gs://workshop-datasets/covertype/data_validation'\n",
    "REGION = 'us-central1'\n",
    "LOCAL_WORKSPACE = '/home/jupyter/workspace'\n",
    "LOCAL_DATA_DIR = os.path.join(LOCAL_WORKSPACE, 'data')\n",
    "BQ_DATASET_NAME = 'data_validation'\n",
    "BQ_TABLE_NAME = 'covertype_classifier_logs'\n",
    "MODEL_NAME = 'covertype_classifier'\n",
    "VERSION_NAME = 'v2'\n",
    "TRAINING_DIR = os.path.join(LOCAL_WORKSPACE, 'training')\n",
    "MODEL_DIR = os.path.join(TRAINING_DIR, 'exported_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32Ps4vjYfaXW"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmYHPLFgEA92"
   },
   "source": [
    "Create a local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XXOcV8BD_w9"
   },
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(LOCAL_WORKSPACE):\n",
    "  print(\"Removing previous workspace artifacts...\")\n",
    "  tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "\n",
    "print(\"Creating a new workspace...\")\n",
    "tf.io.gfile.makedirs(LOCAL_WORKSPACE)\n",
    "tf.io.gfile.makedirs(LOCAL_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLUHs-gh_FnY"
   },
   "source": [
    "## 1. Dataset preparation and schema generation\n",
    "\n",
    "We use the [covertype](https://archive.ics.uci.edu/ml/datasets/covertype) from UCI Machine Learning Repository. The task is to Predict forest cover type from cartographic variables only. \n",
    "\n",
    "The dataset is preprocessed, split, and uploaded to uploaded to the `gs://workshop-datasets/covertype` public GCS location. \n",
    "\n",
    "We use this version of the preprocessed dataset in this notebook. For more information, see [Cover Type Dataset](https://github.com/GoogleCloudPlatform/mlops-on-gcp/tree/master/datasets/covertype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDH43VPLCCsS"
   },
   "source": [
    "### 1.1. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VK3t6GswFJaa"
   },
   "outputs": [],
   "source": [
    "LOCAL_TRAIN_DATA = os.path.join(LOCAL_DATA_DIR, 'train.csv') \n",
    "LOCAL_EVAL_DATA = os.path.join(LOCAL_DATA_DIR, 'eval.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIjykELMB-Zx"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://workshop-datasets/covertype/data_validation/training/dataset.csv {LOCAL_TRAIN_DATA}\n",
    "!gsutil cp gs://workshop-datasets/covertype/data_validation/evaluation/dataset.csv {LOCAL_EVAL_DATA}\n",
    "!wc -l {LOCAL_TRAIN_DATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MV0j0nAezO_R"
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(LOCAL_TRAIN_DATA).head()\n",
    "sample.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUjRurNblnaw"
   },
   "source": [
    "### 1.2 Define metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72rDTLhcUxj_"
   },
   "outputs": [],
   "source": [
    "HEADER = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology',\n",
    "          'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "          'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area', 'Soil_Type',\n",
    "          'Cover_Type']\n",
    "\n",
    "TARGET_FEATURE_NAME = 'Cover_Type'\n",
    "\n",
    "FEATURE_LABELS = ['0', '1', '2', '3', '4', '5', '6']\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['Aspect', 'Elevation', 'Hillshade_3pm', \n",
    "                         'Hillshade_9am', 'Hillshade_Noon', \n",
    "                         'Horizontal_Distance_To_Fire_Points',\n",
    "                         'Horizontal_Distance_To_Hydrology',\n",
    "                         'Horizontal_Distance_To_Roadways','Slope',\n",
    "                         'Vertical_Distance_To_Hydrology']\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    'Soil_Type': ['2702', '2703', '2704', '2705', '2706', '2717', '3501', '3502', \n",
    "                  '4201', '4703', '4704', '4744', '4758', '5101', '6101', '6102', \n",
    "                  '6731', '7101', '7102', '7103', '7201', '7202', '7700', '7701', \n",
    "                  '7702', '7709', '7710', '7745', '7746', '7755', '7756', '7757', \n",
    "                  '7790', '8703', '8707', '8708', '8771', '8772', '8776'], \n",
    "    'Wilderness_Area': ['Cache', 'Commanche', 'Neota', 'Rawah']\n",
    "}\n",
    "\n",
    "FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys()) + NUMERIC_FEATURE_NAMES\n",
    "\n",
    "HEADER_DEFAULTS = [[0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else ['NA'] \n",
    "                   for feature_name in HEADER]\n",
    "\n",
    "NUM_CLASSES = len(FEATURE_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20wwieKoJvL1"
   },
   "source": [
    "## 2. Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e3lk9FsJ_yO"
   },
   "source": [
    "### 2.1. Implement data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTX-t2HjJ_BP"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 19830610\n",
    "import multiprocessing\n",
    "\n",
    "def create_dataset(file_pattern, \n",
    "                  batch_size=128, num_epochs=1, shuffle=False):\n",
    "  \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=HEADER,\n",
    "        column_defaults=HEADER_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        field_delim=',',\n",
    "        header=True,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        shuffle_buffer_size=(5 * batch_size),\n",
    "        shuffle_seed=RANDOM_SEED,\n",
    "        num_parallel_reads=multiprocessing.cpu_count(),\n",
    "        sloppy=True,\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGcaJmcG3paA"
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "for batch in create_dataset(LOCAL_TRAIN_DATA, batch_size=5, shuffle=False).take(2):\n",
    "  print(\"Batch: {}\".format(index))\n",
    "  print(\"========================\")\n",
    "  record, target = batch\n",
    "  print(\"Input features:\")\n",
    "  for key in record:\n",
    "    print(\" - {}:{}\".format(key, record[key].numpy()))\n",
    "  print(\"Target: {}\".format(target))\n",
    "  index += 1\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8s8a7bMGKLWu"
   },
   "source": [
    "### 2.2. Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8pZSfQPHMrK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_feature_columns():\n",
    "  feature_columns = []\n",
    "  \n",
    "  for feature_name in FEATURE_NAMES:\n",
    "    # Categorical features\n",
    "    if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "      \n",
    "      vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "      vocab_size = len(vocabulary)\n",
    "      \n",
    "      # Create embedding column for categotical feature column with vocabulary\n",
    "      embedding_feature_column = tf.feature_column.embedding_column(\n",
    "          categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "              key=feature_name,\n",
    "              vocabulary_list=vocabulary), dimension=int(math.sqrt(vocab_size) + 1))\n",
    "            \n",
    "      feature_columns.append(embedding_feature_column)\n",
    "\n",
    "    # Numeric features\n",
    "    else:\n",
    "      numeric_column = tf.feature_column.numeric_column(feature_name)\n",
    "      feature_columns.append(numeric_column)\n",
    "\n",
    "  return feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8XDvPsq8-mX"
   },
   "outputs": [],
   "source": [
    "feature_columns = create_feature_columns()\n",
    "\n",
    "for column in feature_columns:\n",
    "  print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0pAN0jkKNmm"
   },
   "source": [
    "### 2.3. Create and compile the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E98fiPtQKTp2"
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "\n",
    "  feature_columns = create_feature_columns()\n",
    "  \n",
    "  layers = []\n",
    "  layers.append(tf.keras.layers.DenseFeatures(feature_columns))\n",
    "  for units in params.hidden_units:\n",
    "    layers.append(tf.keras.layers.Dense(units=units, activation='relu'))\n",
    "    layers.append(tf.keras.layers.BatchNormalization())\n",
    "    layers.append(tf.keras.layers.Dropout(rate=params.dropout))\n",
    "  \n",
    "  layers.append(tf.keras.layers.Dense(units=NUM_CLASSES, activation='softmax', name='output'))\n",
    "  \n",
    "  model = tf.keras.Sequential(layers=layers, name='classifier')\n",
    "    \n",
    "  adam_optimzer = tf.keras.optimizers.Adam(learning_rate=params.learning_rate)\n",
    "\n",
    "  model.compile(\n",
    "        optimizer=adam_optimzer, \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()], \n",
    "        loss_weights=None,\n",
    "        sample_weight_mode=None, \n",
    "        weighted_metrics=None, \n",
    "    )\n",
    "\n",
    "  return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjzFWQnSJFse"
   },
   "source": [
    "### 2.4. Train and evaluate experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9_yXPvF2Hkd"
   },
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9iW5NumKWWb"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model, params):\n",
    "\n",
    "  # TensorBoard callback\n",
    "  LOG_DIR = os.path.join(TRAINING_DIR, 'logs')\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "\n",
    "  # Early stopping callback\n",
    "  earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "      monitor='val_sparse_categorical_accuracy', \n",
    "      patience=3, \n",
    "      restore_best_weights=True\n",
    "  )\n",
    "\n",
    "  callbacks = [\n",
    "        tensorboard_callback,\n",
    "        earlystopping_callback]\n",
    "\n",
    "  # train dataset\n",
    "  train_dataset = create_dataset(\n",
    "      LOCAL_TRAIN_DATA,\n",
    "      batch_size=params.batch_size,\n",
    "      shuffle=True)\n",
    "    \n",
    "  # eval dataset\n",
    "  eval_dataset = create_dataset(\n",
    "      LOCAL_EVAL_DATA,\n",
    "      batch_size=params.batch_size)\n",
    "    \n",
    "  # Prep training directory\n",
    "  if tf.io.gfile.exists(TRAINING_DIR):\n",
    "    print(\"Removing previous training artefacts...\")\n",
    "    tf.io.gfile.rmtree(TRAINING_DIR)\n",
    "\n",
    "  print(\"Creating training directory...\")\n",
    "  tf.io.gfile.mkdir(TRAINING_DIR)\n",
    "\n",
    "  print(\"Experiment started...\")\n",
    "  print(\".......................................\")\n",
    "  \n",
    "  # Run train and evaluate.\n",
    "  history = model.fit(\n",
    "    x=train_dataset, \n",
    "    epochs=params.epochs, \n",
    "    callbacks=callbacks,\n",
    "    validation_data=eval_dataset,\n",
    "  )\n",
    "\n",
    "  print(\".......................................\")\n",
    "  print(\"Experiment finished.\")\n",
    "  print(\"\")\n",
    "\n",
    "  return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhYvbrrqJKmn"
   },
   "source": [
    "#### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBCwS_usHvqy"
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    pass\n",
    "\n",
    "TRAIN_DATA_SIZE = 431010\n",
    "\n",
    "params = Parameters()\n",
    "params.learning_rate = 0.01\n",
    "params.hidden_units = [128, 128]\n",
    "params.dropout = 0.15\n",
    "params.batch_size =  265\n",
    "params.steps_per_epoch = int(math.ceil(TRAIN_DATA_SIZE / params.batch_size))\n",
    "params.epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf7FGOscJZ2P"
   },
   "source": [
    "#### Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ew99zH72JRjp"
   },
   "outputs": [],
   "source": [
    "model = create_model(params)\n",
    "example_batch, _ = list(\n",
    "    create_dataset(LOCAL_TRAIN_DATA, batch_size=2, shuffle=True).take(1))[0]\n",
    "model(example_batch)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0DjaL7eI8Ro"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "history = run_experiment(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhJ5qV0aJd0J"
   },
   "source": [
    "#### Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ie7fi-q_JAhh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Eval'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Eval'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7q4uSu1KbXv"
   },
   "source": [
    "## 3. Model export for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwDylJzoUTb1"
   },
   "source": [
    "### 3.1. Implement serving input receiver functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhYjXNxgjob1"
   },
   "source": [
    "#### Feature spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D577fVOKbcw"
   },
   "outputs": [],
   "source": [
    "feature_spec = {}\n",
    "for feature_name in FEATURE_NAMES:\n",
    "    if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "        feature_spec[feature_name] = tf.io.FixedLenFeature(\n",
    "            shape=(1,), dtype=tf.string)\n",
    "    else:\n",
    "        feature_spec[feature_name] = tf.io.FixedLenFeature(\n",
    "            shape=(1,), dtype=tf.float32)\n",
    "\n",
    "for key, value in feature_spec.items():\n",
    "  print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iijp9_mV0gNn"
   },
   "source": [
    "#### Features serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQDJP47xnt9p"
   },
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_KEY = 'probabilities'\n",
    "SKIP_INPUTS = ['request_timestamp']\n",
    "\n",
    "def make_features_serving_fn(model):\n",
    "\n",
    "  @tf.function\n",
    "  def serve_features_fn(features):\n",
    "\n",
    "    # implement any required preprocessing here\n",
    "\n",
    "    return {MODEL_OUTPUT_KEY: model(features)}\n",
    "\n",
    "  return serve_features_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sPh4f_tUX5n"
   },
   "source": [
    "### 3.2. Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlQfiluTTuW-"
   },
   "outputs": [],
   "source": [
    "SIGNATURE_NAME = 'serving_default'\n",
    "\n",
    "features_input_signature = {\n",
    "    feature: tf.TensorSpec(shape=spec.shape, dtype=spec.dtype, name=feature)\n",
    "    for feature, spec in feature_spec.items()\n",
    "    }\n",
    "\n",
    "for key in SKIP_INPUTS:\n",
    "  features_input_signature[key] = tf.TensorSpec(\n",
    "    shape=[None], dtype=tf.string, name=key)\n",
    "\n",
    "signatures = {        \n",
    "    SIGNATURE_NAME: make_features_serving_fn(model).get_concrete_function(\n",
    "        features_input_signature),\n",
    "    }\n",
    "\n",
    "model.save(MODEL_DIR, save_format='tf', signatures=signatures)\n",
    "print(\"Model is exported to: {}.\".format(MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyK7Yy0CCr1f"
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {MODEL_DIR} --tag_set serve --signature_def {SIGNATURE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AQdyoPZUaKH"
   },
   "source": [
    "### 3.3. Test exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gfRf20JVgn6"
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "      { 'request_timestamp': '2020-01-01',\n",
    "        'Soil_Type': '7202',\n",
    "        'Wilderness_Area': 'Commanche',\n",
    "        'Aspect': 61,\n",
    "        'Elevation': 3091,\n",
    "        'Hillshade_3pm': 129,\n",
    "        'Hillshade_9am': 227,\n",
    "        'Hillshade_Noon': 223,\n",
    "        'Horizontal_Distance_To_Fire_Points': 2868,\n",
    "        'Horizontal_Distance_To_Hydrology': 134,\n",
    "        'Horizontal_Distance_To_Roadways': 0, \n",
    "        'Slope': 8, \n",
    "        'Vertical_Distance_To_Hydrology': 10,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IV07vVr1uBvr"
   },
   "source": [
    "#### Preidct using features_serving signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHuRrUjvuK0R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_tf_features(instance):\n",
    " \n",
    "  new_instance = {}\n",
    "  for key, value in instance.items():\n",
    "    if key in CATEGORICAL_FEATURES_WITH_VOCABULARY or key in SKIP_INPUTS:\n",
    "      new_instance[key] = tf.constant(value, dtype=tf.string)\n",
    "    else:\n",
    "      new_instance[key] = tf.constant(value, dtype=tf.float32)\n",
    "  \n",
    "  return new_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUaWFXqgu96i"
   },
   "outputs": [],
   "source": [
    "features_predictor = tf.saved_model.load(MODEL_DIR).signatures[SIGNATURE_NAME]\n",
    "\n",
    "def predict_features_serving(instance):\n",
    "\n",
    "  features = create_tf_features(instance)\n",
    "  probabilities = features_predictor(**features)[MODEL_OUTPUT_KEY].numpy()\n",
    "  predictions = FEATURE_LABELS[int(np.argmax(probabilities))]\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2obiJWB20BX"
   },
   "outputs": [],
   "source": [
    "predict_features_serving(instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fY-3txmZb0kf"
   },
   "source": [
    "### 3.4  Upload exported model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6X0wsuJb0rB"
   },
   "outputs": [],
   "source": [
    "!gsutil rm -r gs://{BUCKET}/models/{MODEL_NAME}\n",
    "!gsutil cp -r {MODEL_DIR} gs://{BUCKET}/models/{MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvVcRazvL_95"
   },
   "source": [
    "## 4. Model deployment to AI Platform \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uncXaPUBdBpV"
   },
   "source": [
    "### 4.1. Create model in AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bO5a1fGLLrSv"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --regions {REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhuGqxSkZs94"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform models list --project {PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmUPxnCZdJZ4"
   },
   "source": [
    "### 4.2. Create a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW-zQi9bZzF5"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --model={MODEL_NAME} \\\n",
    "  --origin=gs://{BUCKET}/models/{MODEL_NAME} \\\n",
    "  --runtime-version=2.1 \\\n",
    "  --framework=TENSORFLOW \\\n",
    "  --python-version=3.7 \\\n",
    "  --project={PROJECT}\n",
    "\n",
    "!gcloud ai-platform versions list --model={MODEL_NAME} --project={PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmFimRVrc4m8"
   },
   "source": [
    "### 4.3. Test deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJJDQNMzc4xh"
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT, MODEL_NAME, VERSION_NAME)\n",
    "print(\"Service name: {}\".format(name))\n",
    "\n",
    "def caip_predict(instances):\n",
    "  \n",
    "  serving_instances = []\n",
    "  for instance in instances:\n",
    "    serving_instances.append(\n",
    "        {key: [value] for key, value in instance.items()})\n",
    "    \n",
    "  request_body={\n",
    "      'signature_name': SIGNATURE_NAME,\n",
    "      'instances': serving_instances}\n",
    "\n",
    "  response = service.projects().predict(\n",
    "      name=name,\n",
    "      body=request_body\n",
    "\n",
    "  ).execute()\n",
    "\n",
    "  if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "\n",
    "  probability_list = [output[MODEL_OUTPUT_KEY] for output in response['predictions']]\n",
    "  classes = [FEATURE_LABELS[int(np.argmax(probabilities))] for probabilities in probability_list]\n",
    "  return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbgnmaks4UPC"
   },
   "outputs": [],
   "source": [
    "caip_predict(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m11-MCflLjFG"
   },
   "source": [
    "## 5. BigQuery logging dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmWQwyAUdwIW"
   },
   "source": [
    "### 5.1. Create BQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ne38zMEKL_Gk"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(PROJECT)\n",
    "dataset_names = [dataset.dataset_id for dataset in client.list_datasets(PROJECT)]\n",
    "\n",
    "dataset = bigquery.Dataset(\"{}.{}\".format(PROJECT, BQ_DATASET_NAME))\n",
    "dataset.location = \"US\"\n",
    "\n",
    "if BQ_DATASET_NAME not in dataset_names:\n",
    "  dataset = client.create_dataset(dataset)\n",
    "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "print(\"BigQuery Dataset is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieIgLE94PovQ"
   },
   "source": [
    "### 5.2. Create BQ Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ima0Dg1UWkRO"
   },
   "source": [
    "#### Table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqxS6RQ0T9LG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "table_schema_json = [\n",
    "  {\n",
    "    \"name\": \"model\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "   },\n",
    "   {\n",
    "     \"name\":\"model_version\", \n",
    "     \"type\": \"STRING\", \n",
    "     \"mode\":\"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"time\", \n",
    "    \"type\": \"TIMESTAMP\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"raw_data\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"raw_prediction\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"NULLABLE\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"groundtruth\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"NULLABLE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "json.dump(table_schema_json, open('table_schema.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koE5tKouWngb"
   },
   "source": [
    "#### Ceating an ingestion-time partitioned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXkoAGEiXRvj"
   },
   "outputs": [],
   "source": [
    "table = bigquery.Table(\n",
    "    \"{}.{}.{}\".format(PROJECT, BQ_DATASET_NAME, BQ_TABLE_NAME))\n",
    "\n",
    "table_names = [table.table_id for table in client.list_tables(dataset)]\n",
    "\n",
    "if BQ_TABLE_NAME in table_names:\n",
    "  print(\"Deleteing BQ Table: {} ...\".format(BQ_TABLE_NAME))\n",
    "  client.delete_table(table)\n",
    "\n",
    "# table = client.create_table(table)\n",
    "# table.partition_expiration = 60 * 60 * 24 * 7\n",
    "# print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnE9GMjbeqXR"
   },
   "outputs": [],
   "source": [
    "TIME_PARTITION_EXPERIATION = 60 * 60 * 24 * 7 # week\n",
    "\n",
    "!bq mk --table \\\n",
    "  --project_id={PROJECT} \\\n",
    "  --time_partitioning_type=DAY \\\n",
    "  --time_partitioning_expiration {TIME_PARTITION_EXPERIATION} \\\n",
    "  {PROJECT}:{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \\\n",
    "  'table_schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPFM4mW-R3y8"
   },
   "source": [
    "### 5.3. Configre the AI Platform model version to enable request-response logging to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBkx4i9Rc51W"
   },
   "outputs": [],
   "source": [
    "sampling_percentage = 1.0\n",
    "bq_full_table_name = '{}.{}.{}'.format(PROJECT, BQ_DATASET_NAME, BQ_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EarF0cmcRZN4"
   },
   "outputs": [],
   "source": [
    "logging_config = {\n",
    "    \"requestLoggingConfig\":{\n",
    "        \"samplingPercentage\": sampling_percentage,\n",
    "        \"bigqueryTableName\": bq_full_table_name\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = service.projects().models().versions().patch(\n",
    "    name=name,\n",
    "    body=logging_config,\n",
    "    updateMask=\"requestLoggingConfig\"\n",
    "    ).execute()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVpICSuq0Kp1"
   },
   "source": [
    "### 5.4. Test request-response logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YsfjkPtkx3o"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "  caip_predict(instances)\n",
    "  print('.', end='')\n",
    "  time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdlrKX0d0mQC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 3/3 [00:00<00:00,  8.20rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>covertype_classifier</td>\n",
       "      <td>covertype_classifier</td>\n",
       "      <td>covertype_classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_version</th>\n",
       "      <td>v2</td>\n",
       "      <td>v2</td>\n",
       "      <td>v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2020-04-30 22:34:59+00:00</td>\n",
       "      <td>2020-04-30 22:34:59+00:00</td>\n",
       "      <td>2020-04-30 22:34:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_data</th>\n",
       "      <td>{\"signature_name\": \"serving_default\", \"instanc...</td>\n",
       "      <td>{\"signature_name\": \"serving_default\", \"instanc...</td>\n",
       "      <td>{\"signature_name\": \"serving_default\", \"instanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_prediction</th>\n",
       "      <td>{\"predictions\": [{\"probabilities\": [0.02607797...</td>\n",
       "      <td>{\"predictions\": [{\"probabilities\": [0.02607797...</td>\n",
       "      <td>{\"predictions\": [{\"probabilities\": [0.02607797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groundtruth</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0  \\\n",
       "model                                        covertype_classifier   \n",
       "model_version                                                  v2   \n",
       "time                                    2020-04-30 22:34:59+00:00   \n",
       "raw_data        {\"signature_name\": \"serving_default\", \"instanc...   \n",
       "raw_prediction  {\"predictions\": [{\"probabilities\": [0.02607797...   \n",
       "groundtruth                                                  None   \n",
       "\n",
       "                                                                1  \\\n",
       "model                                        covertype_classifier   \n",
       "model_version                                                  v2   \n",
       "time                                    2020-04-30 22:34:59+00:00   \n",
       "raw_data        {\"signature_name\": \"serving_default\", \"instanc...   \n",
       "raw_prediction  {\"predictions\": [{\"probabilities\": [0.02607797...   \n",
       "groundtruth                                                  None   \n",
       "\n",
       "                                                                2  \n",
       "model                                        covertype_classifier  \n",
       "model_version                                                  v2  \n",
       "time                                    2020-04-30 22:34:58+00:00  \n",
       "raw_data        {\"signature_name\": \"serving_default\", \"instanc...  \n",
       "raw_prediction  {\"predictions\": [{\"probabilities\": [0.02607797...  \n",
       "groundtruth                                                  None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "  SELECT * FROM \n",
    "  `{}.{}` \n",
    "  WHERE model_version = '{}'\n",
    "  ORDER BY time desc\n",
    "  LIMIT {}\n",
    "'''.format(BQ_DATASET_NAME, BQ_TABLE_NAME, VERSION_NAME, 3)\n",
    "\n",
    "pd.io.gbq.read_gbq(\n",
    "    query, project_id=PROJECT).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vK_yvIjWkPk7"
   },
   "source": [
    "## 6. BigQuery logs parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fTqQkXEJszV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FEATURE_NAMES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a11ae667bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mview_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vw_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBQ_TABLE_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mVERSION_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcolum_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'request_timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFEATURE_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minput_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', \\r\\n  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolum_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FEATURE_NAMES' is not defined"
     ]
    }
   ],
   "source": [
    "view_name = \"vw_\"+BQ_TABLE_NAME+\"_\"+VERSION_NAME\n",
    "\n",
    "colum_names = ['request_timestamp'] + FEATURE_NAMES\n",
    "input_features = ', \\r\\n  '.join(colum_names)\n",
    "\n",
    "json_features_extraction = []\n",
    "for feature_name in colum_names:\n",
    "  s = \"JSON_EXTRACT(instance, '$.{}')\".format(feature_name) \n",
    "  if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "    s = \"CAST({} AS NUMERIC)\".format(s)\n",
    "  s += \" AS {}\".format(feature_name)\n",
    "  json_features_extraction.append(s)\n",
    "json_features_extraction = ', \\r\\n    '.join(json_features_extraction)\n",
    "\n",
    "class_probability_pivoting = []\n",
    "for class_index, class_label in enumerate(FEATURE_LABELS):\n",
    "  s = \"CAST(MAX(IF(class_index = {}, class_probability, NULL)) as FLOAT64) as prob_{}\".format(class_index, class_label)\n",
    "  class_probability_pivoting.append(s)\n",
    "class_probability_pivoting = ', \\r\\n  '.join(class_probability_pivoting)\n",
    "\n",
    "\n",
    "class_prob = []\n",
    "for class_label in FEATURE_LABELS:\n",
    "  s = 'prob_{}'.format(class_label)\n",
    "  class_prob.append(s)\n",
    "\n",
    "class_prob = ', \\r\\n  '.join(class_prob)\n",
    "\n",
    "case_conditions = []\n",
    "for class_label in FEATURE_LABELS:\n",
    "  s = 'WHEN prob_max = prob_{} THEN {}'.format(class_label, class_label)\n",
    "  case_conditions.append(s)\n",
    "case_conditions = '   \\r\\n '.join(case_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_ArYsvKqn3j"
   },
   "source": [
    "### 6.1. Create a view to parse the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69vABhIJcqbR"
   },
   "outputs": [],
   "source": [
    "sql_script = '''\n",
    "CREATE OR REPLACE VIEW @dataset_name.@view_name\n",
    "AS\n",
    "\n",
    "WITH step1\n",
    "AS\n",
    "(\n",
    "  SELECT \n",
    "    model, \n",
    "    model_version, \n",
    "    time, \n",
    "    SPLIT(JSON_EXTRACT(raw_data, '$.instances'), '}],[{') instance_list, \n",
    "    SPLIT(JSON_EXTRACT(raw_prediction, '$.predictions'), '}],[{') as prediction_list\n",
    "  FROM \n",
    "  `@project.@dataset_name.@table_name` \n",
    "  WHERE model_version = '@version'\n",
    "),\n",
    "\n",
    "step2\n",
    "AS\n",
    "(\n",
    "  SELECT\n",
    "    model, \n",
    "    model_version, \n",
    "    time, \n",
    "    REPLACE(REPLACE(instance, '[', ''),']', '') AS instance,\n",
    "    REPLACE(REPLACE(prediction, '[{\"@model_output_key\":[', ''),']}]', '') AS prediction,\n",
    "  FROM step1\n",
    "  JOIN UNNEST(step1.instance_list) AS instance\n",
    "  WITH OFFSET AS f1\n",
    "  JOIN UNNEST(step1.prediction_list) AS prediction\n",
    "  WITH OFFSET AS f2\n",
    "  ON f1=f2\n",
    "),\n",
    "\n",
    "step3 AS\n",
    "(\n",
    "  SELECT \n",
    "    model, \n",
    "    model_version, \n",
    "    time,\n",
    "    @json_features_extraction,\n",
    "    SPLIT(prediction, ',') AS class_probabilities, \n",
    "  FROM step2\n",
    "),\n",
    "\n",
    "step4\n",
    "AS\n",
    "(\n",
    "  SELECT * EXCEPT(class_probabilities)\n",
    "  FROM step3\n",
    "  JOIN UNNEST(step3.class_probabilities) AS class_probability\n",
    "  WITH OFFSET AS class_index\n",
    "),\n",
    "\n",
    "step5\n",
    "AS\n",
    "(\n",
    "  SELECT\n",
    "    model,\n",
    "    model_version,\n",
    "    time,\n",
    "    @input_features,\n",
    "    @class_probability_pivoting,\n",
    "    MAX(CAST(class_probability AS FLOAT64)) as prob_max\n",
    "  FROM step4\n",
    "  GROUP BY\n",
    "    model,\n",
    "    model_version,\n",
    "    time,\n",
    "    @input_features\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  model,\n",
    "  model_version,\n",
    "  time,\n",
    "  @input_features,\n",
    "  @class_prob,\n",
    "  CASE\n",
    "  @case_conditions\n",
    "  END as predicted_class\n",
    "FROM\n",
    "  step5\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWIlgsxRI4wV"
   },
   "outputs": [],
   "source": [
    "sql_script = sql_script.replace(\"@project\", PROJECT)\n",
    "sql_script = sql_script.replace(\"@dataset_name\", BQ_DATASET_NAME)\n",
    "sql_script = sql_script.replace(\"@table_name\", BQ_TABLE_NAME)\n",
    "sql_script = sql_script.replace(\"@view_name\", view_name)\n",
    "sql_script = sql_script.replace(\"@version\", VERSION_NAME)\n",
    "sql_script = sql_script.replace(\"@input_features\", input_features)\n",
    "sql_script = sql_script.replace(\"@json_features_extraction\", json_features_extraction)\n",
    "sql_script = sql_script.replace(\"@model_output_key\", MODEL_OUTPUT_KEY)\n",
    "sql_script = sql_script.replace(\"@class_probability_pivoting\", class_probability_pivoting)\n",
    "sql_script = sql_script.replace(\"@class_prob\", class_prob)\n",
    "sql_script = sql_script.replace(\"@case_conditions\", case_conditions)\n",
    "#print(sql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZwFA1OGUMPz"
   },
   "outputs": [],
   "source": [
    "client.query(query = sql_script)\n",
    "print(\"View was created or replaced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6qXk_QjUMHq"
   },
   "source": [
    "### 6.2. Query the view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9WDwhQbU0qX"
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT * FROM \n",
    "  `{}.{}` \n",
    "  LIMIT {}\n",
    "'''.format(BQ_DATASET_NAME, view_name, 3)\n",
    "\n",
    "pd.io.gbq.read_gbq(\n",
    "    query, project_id=PROJECT).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkanQP8yVYaL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "20wwieKoJvL1",
    "gvVcRazvL_95",
    "m11-MCflLjFG",
    "HmWQwyAUdwIW",
    "ieIgLE94PovQ",
    "RPFM4mW-R3y8"
   ],
   "name": "00-covertype-training-serving-v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
