{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFDV and Apache Beam sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "from typing import List, Optional, Text, Union, Dict, Iterable\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "\n",
    "#from tensorflow_data_validation import types\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(flags=[])\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow will run.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-central1'\n",
    "\n",
    "# Because this notebook comes with a locally built version of the Beam Python SDK, we will need to set\n",
    "# the sdk_location option for the Dataflow Runner. You will not need to do this if you are using an\n",
    "# officially released version of Apache Beam.\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)\n",
    "\n",
    "# IMPORTANT! Please adjust the following to choose a GCS location.\n",
    "dataflow_gcs_location = 'gs://<my_bucket>/dataflow'\n",
    "\n",
    "# Dataflow Staging Location.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow Temp Location.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
    "\n",
    "p = beam.Pipeline(DataflowRunner(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "select * from `covertype_dataset.covertype`\n",
    "limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDriftOptions(PipelineOptions):\n",
    "    @classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "        '--baseline_data_location',\n",
    "        help=\"Input for the pipeline\",\n",
    "        default='gs://workshop-datasets/covertype/small')\n",
    "        \n",
    "        parser.add_argument(\n",
    "        '--target_data_table',\n",
    "        help=\"A target BQ table with data to analyze\",\n",
    "        default=\"covertype_dataset.covertype\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "  \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "  sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "  query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "  return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table_name = 'mlops-dev-env.covertype_dataset.covertype'\n",
    "num_lots = 100\n",
    "lots = [1]\n",
    "\n",
    "query = generate_sampling_query(source_table_name, num_lots, lots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "options.view_as(GoogleCloudOptions).project = 'mlops-dev-env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_options = options.view_as(DataDriftOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedDictsToRecordBatch(beam.DoFn):\n",
    "    \"\"\"DoFn to convert a batch of dictionaries to a pyarrow.RecordBatch.\n",
    "  \n",
    "    In the current implementation, the function relies on automatic\n",
    "    coercion to pyarrow data types. \n",
    "    To be verified whether this is a right approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def process(self, batch: List[Dict]) -> Iterable[pa.RecordBatch]:\n",
    "        \n",
    "        column_names = batch[0].keys()\n",
    "        values_by_column = {column_name: [] for column_name in column_names}\n",
    "        for row in batch:\n",
    "            for key, value in row.items():\n",
    "                values_by_column[key].append(value)\n",
    "                \n",
    "        arrow_arrays = [\n",
    "            pa.array(arr) for arr in values_by_column.values()\n",
    "            ]  \n",
    "        \n",
    "        yield pa.RecordBatch.from_arrays(arrow_arrays, list(column_names))\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch = [\n",
    "    {\n",
    "        'c1': 10,\n",
    "        'c2': 'aaa'\n",
    "    },\n",
    "    {\n",
    "        'c1': 20,\n",
    "        'c2': 'bbb'\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "t1 = BatchedDictsToRecordBatch()\n",
    "\n",
    "\n",
    "for item in t1.process(batch):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@beam.typehints.with_input_types(types.BeamCSVRecord)\n",
    "#@beam.typehints.with_output_types(pa.RecordBatch)\n",
    "class DecodeBigQuery(beam.PTransform):\n",
    "    \"\"\"Decodes BigQuery records into Arrow RecordBatches.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        column_names: List):\n",
    "    \n",
    "        if not isinstance(column_names, list):\n",
    "            raise TypeError('column_names is of type %s, should be a list' %\n",
    "                            type(column_names).__name__)\n",
    "        self._column_names = column_names\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        record_batches = (\n",
    "            pcoll\n",
    "            | beam.Map(lambda row: {field: row[field]\n",
    "                                    for field in self._column_names})\n",
    "            | beam.BatchElements(min_batch_size=2, max_batch_size=3)\n",
    "            | beam.ParDo(\n",
    "                BatchedDictsToRecordBatch())\n",
    "            )\n",
    "\n",
    "        return record_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = (p\n",
    "           | 'query' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
    "           | 'batch' >> DecodeBigQuery(['Elevation', 'Aspect'])\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (p \n",
    "         | 'read' >> beam.io.ReadFromText(custom_options.data_location)\n",
    "         | 'extract' >> beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (words \n",
    "          | 'count' >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'a': 2, 'b': 'hello'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query_job = client.query('SELECT * FROM ({}) LIMIT 0'.format(query))\n",
    "results = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {field.name: field.field_type for field in results.schema}\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'BOOL' in schema.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_types = set(['INTEGER', 'STRING', 'BOOL'])\n",
    "supported_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = set(['STRING', 'BOOL', 'STRING', 'INTEGER', 'FLOAT'])\n",
    "query_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema in supported_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema.issubset(supported_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(schema.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = ['aa', 'bb', 'cc']\n",
    "lb = ['cc', 'bb', 'aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(la) == set(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(schema.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jupyter/artifact_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jupyter/artifact_store/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.load_statistics(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
