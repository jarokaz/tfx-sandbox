{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and deploy an sklearn CoverType model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.mlops-dev-env.appspot.com/\n",
      "gs://dataflow-staging-us-central1-881178567352/\n",
      "gs://hostedkfp-default-36un4wco1q/\n",
      "gs://hostedkfp-default-w2dc42i8jo/\n",
      "gs://jk-mlops-dev-sandbox/\n",
      "gs://mlops-dev-env-staging/\n",
      "gs://mlops-dev-env.appspot.com/\n",
      "gs://mlops-dev-env_cloudbuild/\n",
      "gs://mlops-dev-workspace/\n",
      "gs://staging.mlops-dev-env.appspot.com/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://mlops-dev-workspace/artifact-store'\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "\n",
    "DATA_ROOT='gs://workshop-datasets/covertype/data_validation'\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.csv')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'evaluation', 'dataset.csv')\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431009, 13)\n",
      "(75000, 13)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>3034</td>\n",
       "      <td>2827</td>\n",
       "      <td>2854</td>\n",
       "      <td>3188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>235</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>655</td>\n",
       "      <td>85</td>\n",
       "      <td>484</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>134</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>5799</td>\n",
       "      <td>3511</td>\n",
       "      <td>618</td>\n",
       "      <td>3666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>197</td>\n",
       "      <td>211</td>\n",
       "      <td>215</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>251</td>\n",
       "      <td>204</td>\n",
       "      <td>214</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>190</td>\n",
       "      <td>125</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>4094</td>\n",
       "      <td>6008</td>\n",
       "      <td>1295</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <td>Rawah</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>Commanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type</th>\n",
       "      <td>7745</td>\n",
       "      <td>7745</td>\n",
       "      <td>4704</td>\n",
       "      <td>7756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cover_Type</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0      1          2          3\n",
       "Elevation                            3034   2827       2854       3188\n",
       "Aspect                                235     26         27        156\n",
       "Slope                                  13     16         12         13\n",
       "Horizontal_Distance_To_Hydrology      655     85        484        175\n",
       "Vertical_Distance_To_Hydrology        134     13         22         28\n",
       "Horizontal_Distance_To_Roadways      5799   3511        618       3666\n",
       "Hillshade_9am                         197    211        215        235\n",
       "Hillshade_Noon                        251    204        214        241\n",
       "Hillshade_3pm                         190    125        134        134\n",
       "Horizontal_Distance_To_Fire_Points   4094   6008       1295       1719\n",
       "Wilderness_Area                     Rawah  Rawah  Commanche  Commanche\n",
       "Soil_Type                            7745   7745       4704       7756\n",
       "Cover_Type                              1      1          1          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(4).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the `sklearn` training pipeline.\n",
    "\n",
    "The training pipeline preprocesses data by standardizing all numeric features using `sklearn.preprocessing.StandardScaler` and encoding all categorical features using `sklearn.preprocessing.OneHotEncoder`. It uses stochastic gradient descent linear classifier (`SGDClassifier`) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_indexes = slice(0, 10)\n",
    "categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log', tol=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  slice(0, 10, None)),\n",
       "                                                 ('cat',\n",
       "                                                  OneHotEncoder(categorical_features=None,\n",
       "                                                                categories=None,\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='e...\n",
       "                ('classifier',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='log',\n",
       "                               max_iter=2000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=2000)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trained model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042133333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_model_path = '%s/model/' % ARTIFACT_STORE\n",
    "local_model_path = '/tmp/model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/model.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  6.0 KiB/  6.0 KiB]                                                \n",
      "Operation completed over 1 objects/6.0 KiB.                                      \n",
      "gs://mlops-dev-workspace/artifact-store/model/model.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(local_model_path, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        \n",
    "!gsutil cp {local_model_path} {gcs_model_path}\n",
    "!gsutil ls {gcs_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to AI Platform Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/mlops-dev-env/models/covertype_classifier_sklearn].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'covertype_classifier_sklearn'\n",
    "labels = \"task=classifier,domain=forestry\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)' --quiet)\n",
    "models = [model for model in models if model[:7] != 'WARNING']\n",
    "\n",
    "if not models:\n",
    "    !gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "else:\n",
    "    print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "model_version = 'v1'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "versions = [version for version in versions if version[:7] != 'WARNING']\n",
    "\n",
    "if not versions:\n",
    "    !gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin={gcs_model_path} \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "else:\n",
    "    print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "v1\n"
     ]
    }
   ],
   "source": [
    "!(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service name: projects/mlops-dev-env/models/covertype_classifier_sklearn/versions/v1\n"
     ]
    }
   ],
   "source": [
    "service = discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, model_name, model_version)\n",
    "print(\"Service name: {}\".format(name))\n",
    "\n",
    "def caip_predict(instances):\n",
    "    \n",
    "  request_body={\n",
    "      'instances': instances}\n",
    "\n",
    "  response = service.projects().predict(\n",
    "      name=name,\n",
    "      body=request_body\n",
    "\n",
    "  ).execute()\n",
    "\n",
    "  if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [1, 1, 0]}\n",
      "***\n",
      "{'predictions': [0, 1, 1]}\n",
      "***\n",
      "{'predictions': [0, 2, 1]}\n",
      "***\n",
      "{'predictions': [1, 0, 1]}\n",
      "***\n",
      "{'predictions': [1, 1, 0]}\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "instance_iterator = X_validation.iterrows()\n",
    "\n",
    "for num_calls in range(5):\n",
    "    instances = []\n",
    "    for num_instances in range(3):\n",
    "        instances.append(list(next(instance_iterator)[1].values))\n",
    "    response = caip_predict(instances)\n",
    "    print(response)\n",
    "    print(\"***\")\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m11-MCflLjFG"
   },
   "source": [
    "## 5. BigQuery logging dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmWQwyAUdwIW"
   },
   "source": [
    "### 5.1. Create BQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ne38zMEKL_Gk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery Dataset is ready.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "BQ_DATASET_NAME = 'data_validation'\n",
    "BQ_TABLE_NAME = 'covertype_classifier_logs_sklearn'\n",
    "\n",
    "client = bigquery.Client(PROJECT_ID)\n",
    "dataset_names = [dataset.dataset_id for dataset in client.list_datasets(PROJECT_ID)]\n",
    "\n",
    "dataset = bigquery.Dataset(\"{}.{}\".format(PROJECT_ID, BQ_DATASET_NAME))\n",
    "dataset.location = \"US\"\n",
    "\n",
    "if BQ_DATASET_NAME not in dataset_names:\n",
    "  dataset = client.create_dataset(dataset)\n",
    "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "print(\"BigQuery Dataset is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieIgLE94PovQ"
   },
   "source": [
    "### 5.2. Create BQ Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ima0Dg1UWkRO"
   },
   "source": [
    "#### Table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqxS6RQ0T9LG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "table_schema_json = [\n",
    "  {\n",
    "    \"name\": \"model\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "   },\n",
    "   {\n",
    "     \"name\":\"model_version\", \n",
    "     \"type\": \"STRING\", \n",
    "     \"mode\":\"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"time\", \n",
    "    \"type\": \"TIMESTAMP\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"raw_data\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"REQUIRED\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"raw_prediction\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"NULLABLE\"\n",
    "  },\n",
    "  {\n",
    "    \"name\":\"groundtruth\", \n",
    "    \"type\": \"STRING\", \n",
    "    \"mode\": \"NULLABLE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "json.dump(table_schema_json, open('table_schema.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koE5tKouWngb"
   },
   "source": [
    "#### Ceating an ingestion-time partitioned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXkoAGEiXRvj"
   },
   "outputs": [],
   "source": [
    "table = bigquery.Table(\n",
    "    \"{}.{}.{}\".format(PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME))\n",
    "\n",
    "table_names = [table.table_id for table in client.list_tables(dataset)]\n",
    "\n",
    "if BQ_TABLE_NAME in table_names:\n",
    "  print(\"Deleteing BQ Table: {} ...\".format(BQ_TABLE_NAME))\n",
    "  client.delete_table(table)\n",
    "\n",
    "# table = client.create_table(table)\n",
    "# table.partition_expiration = 60 * 60 * 24 * 7\n",
    "# print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnE9GMjbeqXR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'mlops-dev-env:data_validation.covertype_classifier_logs_sklearn' successfully created.\n"
     ]
    }
   ],
   "source": [
    "TIME_PARTITION_EXPERIATION = 60 * 60 * 24 * 7 # week\n",
    "\n",
    "!bq mk --table \\\n",
    "  --project_id={PROJECT_ID} \\\n",
    "  --time_partitioning_type=DAY \\\n",
    "  --time_partitioning_expiration {TIME_PARTITION_EXPERIATION} \\\n",
    "  {PROJECT_ID}:{BQ_DATASET_NAME}.{BQ_TABLE_NAME} \\\n",
    "  'table_schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPFM4mW-R3y8"
   },
   "source": [
    "### 5.3. Configre the AI Platform model version to enable request-response logging to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBkx4i9Rc51W"
   },
   "outputs": [],
   "source": [
    "sampling_percentage = 1.0\n",
    "bq_full_table_name = '{}.{}.{}'.format(PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EarF0cmcRZN4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/operations/update_covertype_classifier_sklearn_v1_1589050819119',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.ml.v1.OperationMetadata',\n",
       "  'createTime': '2020-05-09T19:00:19Z',\n",
       "  'operationType': 'UPDATE_VERSION',\n",
       "  'modelName': 'projects/mlops-dev-env/models/covertype_classifier_sklearn',\n",
       "  'version': {'name': 'projects/mlops-dev-env/models/covertype_classifier_sklearn/versions/v1',\n",
       "   'deploymentUri': 'gs://mlops-dev-workspace/artifact-store/model/',\n",
       "   'createTime': '2020-05-09T18:57:46Z',\n",
       "   'runtimeVersion': '1.15',\n",
       "   'state': 'READY',\n",
       "   'etag': 'a69ff4JaCGU=',\n",
       "   'framework': 'SCIKIT_LEARN',\n",
       "   'machineType': 'mls1-c1-m2',\n",
       "   'pythonVersion': '3.7',\n",
       "   'requestLoggingConfig': {'samplingPercentage': 1,\n",
       "    'bigqueryTableName': 'mlops-dev-env.data_validation.covertype_classifier_logs_sklearn'}}}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_config = {\n",
    "    \"requestLoggingConfig\":{\n",
    "        \"samplingPercentage\": sampling_percentage,\n",
    "        \"bigqueryTableName\": bq_full_table_name\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = service.projects().models().versions().patch(\n",
    "    name=name,\n",
    "    body=logging_config,\n",
    "    updateMask=\"requestLoggingConfig\"\n",
    "    ).execute()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVpICSuq0Kp1"
   },
   "source": [
    "### 5.4. Test request-response logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_iterator = X_validation.iterrows()\n",
    "\n",
    "for num_calls in range(1000):\n",
    "    instances = []\n",
    "    for num_instances in range(3):\n",
    "        instances.append(list(next(instance_iterator)[1].values))\n",
    "    response = caip_predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdlrKX0d0mQC"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pandas_gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas_gbq.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1b5b9ab275f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m result = pd.io.gbq.read_gbq(\n\u001b[0;32m---> 10\u001b[0;31m     query, project_id=PROJECT_ID)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, private_key, verbose)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gbq\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mWrite\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mto\u001b[0m \u001b[0mGoogle\u001b[0m \u001b[0mBigQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mpandas_gbq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36m_try_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"See the docs: https://pandas-gbq.readthedocs.io.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpandas_gbq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas_gbq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpandas_gbq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pandas_gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas_gbq."
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "  SELECT * FROM \n",
    "  `{}.{}` \n",
    "  WHERE model_version = '{}'\n",
    "  ORDER BY time desc\n",
    "  LIMIT {}\n",
    "'''.format(BQ_DATASET_NAME, BQ_TABLE_NAME, model_version, 3)\n",
    "\n",
    "result = pd.io.gbq.read_gbq(\n",
    "    query, project_id=PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"instances\": [[2665, 114, 10, 190, 62, 1634, ...\n",
       "1    {\"instances\": [[2451, 315, 10, 382, 234, 1092,...\n",
       "2    {\"instances\": [[2744, 23, 10, 120, 11, 1566, 2...\n",
       "Name: raw_data, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['raw_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = json.loads(result['raw_data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instances': [[2665,\n",
       "   114,\n",
       "   10,\n",
       "   190,\n",
       "   62,\n",
       "   1634,\n",
       "   238,\n",
       "   230,\n",
       "   121,\n",
       "   1181,\n",
       "   'Commanche',\n",
       "   'C4704'],\n",
       "  [2639, 51, 10, 384, 19, 3132, 225, 219, 127, 2919, 'Commanche', 'C4704'],\n",
       "  [2662, 41, 10, 60, 0, 1440, 221, 219, 132, 1454, 'Commanche', 'C4704']]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
